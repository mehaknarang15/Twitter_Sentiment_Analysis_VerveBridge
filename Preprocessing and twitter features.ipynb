{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439c0b9f-5797-4ff3-9999-cc7b155198dc",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97025e04-7b7b-4842-8291-e6c36211604a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:29: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:62: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:62: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:62: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:62: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_1964\\3954792204.py:29: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  ('__EMOT_LOVE',\t\t['<3', ':\\*', ] )\t,\\\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_1964\\3954792204.py:62: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  return [text.replace(')', '[)}\\]]').replace('(', '[({\\[]') for text in arr]\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_1964\\3954792204.py:62: SyntaxWarning: invalid escape sequence '\\['\n",
      "  return [text.replace(')', '[)}\\]]').replace('(', '[({\\[]') for text in arr]\n"
     ]
    }
   ],
   "source": [
    "# This Python file uses the following encoding: utf-8\n",
    "import re\n",
    "\n",
    "# Hashtags\n",
    "hash_regex = re.compile(r\"#(\\w+)\")\n",
    "def hash_repl(match):\n",
    "\treturn '__HASH_'+match.group(1).upper()\n",
    "\n",
    "# Handels\n",
    "hndl_regex = re.compile(r\"@(\\w+)\")\n",
    "def hndl_repl(match):\n",
    "\treturn '__HNDL'#_'+match.group(1).upper()\n",
    "\n",
    "# URLs\n",
    "url_regex = re.compile(r\"(http|https|ftp)://[a-zA-Z0-9\\./]+\")\n",
    "\n",
    "# Spliting by word boundaries\n",
    "word_bound_regex = re.compile(r\"\\W+\")\n",
    "\n",
    "# Repeating words like hurrrryyyyyy\n",
    "rpt_regex = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE);\n",
    "def rpt_repl(match):\n",
    "\treturn match.group(1)+match.group(1)\n",
    "\n",
    "# Emoticons\n",
    "emoticons = \\\n",
    "\t[\t('__EMOT_SMILEY',\t[':-)', ':)', '(:', '(-:', ] )\t,\\\n",
    "\t\t('__EMOT_LAUGH',\t\t[':-D', ':D', 'X-D', 'XD', 'xD', ] )\t,\\\n",
    "\t\t('__EMOT_LOVE',\t\t['<3', ':\\*', ] )\t,\\\n",
    "\t\t('__EMOT_WINK',\t\t[';-)', ';)', ';-D', ';D', '(;', '(-;', ] )\t,\\\n",
    "\t\t('__EMOT_FROWN',\t\t[':-(', ':(', '(:', '(-:', ] )\t,\\\n",
    "\t\t('__EMOT_CRY',\t\t[':,(', ':\\'(', ':\"(', ':(('] )\t,\\\n",
    "\t]\n",
    "\n",
    "# Punctuations\n",
    "punctuations = \\\n",
    "\t[\t#('',\t\t['.', ] )\t,\\\n",
    "\t\t#('',\t\t[',', ] )\t,\\\n",
    "\t\t#('',\t\t['\\'', '\\\"', ] )\t,\\\n",
    "\t\t('__PUNC_EXCL',\t\t['!', '¡', ] )\t,\\\n",
    "\t\t('__PUNC_QUES',\t\t['?', '¿', ] )\t,\\\n",
    "\t\t('__PUNC_ELLP',\t\t['...', '…', ] )\t,\\\n",
    "\t\t#FIXME : MORE? http://en.wikipedia.org/wiki/Punctuation\n",
    "\t]\n",
    "\n",
    "#Printing functions for info\n",
    "def print_config(cfg):\n",
    "\tfor (x, arr) in cfg:\n",
    "\t\tprint (x, '\\t')\n",
    "\t\tfor a in arr:\n",
    "\t\t\tprint (a, '\\t')\n",
    "\t\tprint ('')\n",
    "\n",
    "def print_emoticons():\n",
    "\tprint_config(emoticons)\n",
    "\n",
    "def print_punctuations():\n",
    "\tprint_config(punctuations)\n",
    "\n",
    "#For emoticon regexes\n",
    "def escape_paren(arr):\n",
    "\treturn [text.replace(')', '[)}\\]]').replace('(', '[({\\[]') for text in arr]\n",
    "\n",
    "def regex_union(arr):\n",
    "\treturn '(' + '|'.join( arr ) + ')'\n",
    "\n",
    "emoticons_regex = [ (repl, re.compile(regex_union(escape_paren(regx))) ) \\\n",
    "\t\t\t\t\tfor (repl, regx) in emoticons ]\n",
    "\n",
    "#For punctuation replacement\n",
    "def punctuations_repl(match):\n",
    "\ttext = match.group(0)\n",
    "\trepl = []\n",
    "\tfor (key, parr) in punctuations :\n",
    "\t\tfor punc in parr :\n",
    "\t\t\tif punc in text:\n",
    "\t\t\t\trepl.append(key)\n",
    "\tif( len(repl)>0 ) :\n",
    "\t\treturn ' '+' '.join(repl)+' '\n",
    "\telse :\n",
    "\t\treturn ' '\n",
    "\n",
    "def processHashtags( \ttext, subject='', query=[]):\n",
    "\treturn re.sub( hash_regex, hash_repl, text )\n",
    "\n",
    "def processHandles( \ttext, subject='', query=[]):\n",
    "\treturn re.sub( hndl_regex, hndl_repl, text )\n",
    "\n",
    "def processUrls( \t\ttext, subject='', query=[]):\n",
    "\treturn re.sub( url_regex, ' __URL ', text )\n",
    "\n",
    "def processEmoticons( \ttext, subject='', query=[]):\n",
    "\tfor (repl, regx) in emoticons_regex :\n",
    "\t\ttext = re.sub(regx, ' '+repl+' ', text)\n",
    "\treturn text\n",
    "\n",
    "def processPunctuations( text, subject='', query=[]):\n",
    "\treturn re.sub( word_bound_regex , punctuations_repl, text )\n",
    "\n",
    "def processRepeatings( \ttext, subject='', query=[]):\n",
    "\treturn re.sub( rpt_regex, rpt_repl, text )\n",
    "\n",
    "def processQueryTerm( \ttext, subject='', query=[]):\n",
    "\tquery_regex = \"|\".join([ re.escape(q) for q in query])\n",
    "\treturn re.sub( query_regex, '__QUER', text, flags=re.IGNORECASE )\n",
    "\n",
    "def countHandles(text):\n",
    "\treturn len( re.findall( hndl_regex, text) )\n",
    "def countHashtags(text):\n",
    "\treturn len( re.findall( hash_regex, text) )\n",
    "def countUrls(text):\n",
    "\treturn len( re.findall( url_regex, text) )\n",
    "def countEmoticons(text):\n",
    "\tcount = 0\n",
    "\tfor (repl, regx) in emoticons_regex :\n",
    "\t\tcount += len( re.findall( regx, text) )\n",
    "\treturn count\n",
    "\n",
    "#FIXME: preprocessing.preprocess()! wtf! will need to move.\n",
    "#FIXME: use process functions inside\n",
    "def processAll( \t\ttext, subject='', query=[]):\n",
    "\n",
    "\tif(len(query)>0):\n",
    "\t\tquery_regex = \"|\".join([ re.escape(q) for q in query])\n",
    "\t\ttext = re.sub( query_regex, '__QUER', text, flags=re.IGNORECASE )\n",
    "\n",
    "\ttext = re.sub( hash_regex, hash_repl, text )\n",
    "\ttext = re.sub( hndl_regex, hndl_repl, text )\n",
    "\ttext = re.sub( url_regex, ' __URL ', text )\n",
    "\n",
    "\tfor (repl, regx) in emoticons_regex :\n",
    "\t\ttext = re.sub(regx, ' '+repl+' ', text)\n",
    "\n",
    "\n",
    "\ttext = text.replace('\\'','')\n",
    "\t# FIXME: Jugad\n",
    "\n",
    "\ttext = re.sub( word_bound_regex , punctuations_repl, text )\n",
    "\ttext = re.sub( rpt_regex, rpt_repl, text )\n",
    "\n",
    "\treturn text\n",
    "\n",
    "#from time import time\n",
    "#import preprocessing, sanderstwitter02\n",
    "#tweets = sanderstwitter02.getTweetsRawData('sentiment.csv')\n",
    "#start = time()\n",
    "#procTweets = [ (preprocessing.preprocess(t),s) for (t,s) in tweets]\n",
    "#end = time()\n",
    "#end - start\n",
    "\n",
    "#uni = [ a if(a[0:2]=='__') else a.lower() for a in re.findall(r\"\\w+\", text) ]\n",
    "#bi  = nltk.bigrams(uni)\n",
    "#tri = nltk.trigrams(uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d3000-1894-457e-a0d0-c1662f0e0dcc",
   "metadata": {},
   "source": [
    "## Twitter Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30e6f907-13dc-40e0-968c-0422b37bd9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@package tweet_features\n",
    "Convert tweet to feature vector.\n",
    "\n",
    "These routines help convert arbitrary tweets in to feature vectors.\n",
    "\n",
    "\"\"\"\n",
    "import numpy\n",
    "\n",
    "\n",
    "# search patterns for features\n",
    "testFeatures = \\\n",
    "    [('hasAddict',     (' addict',)), \\\n",
    "     ('hasAwesome',    ('awesome',)), \\\n",
    "     ('hasBroken',     ('broke',)), \\\n",
    "     ('hasBad',        (' bad',)), \\\n",
    "     ('hasBug',        (' bug',)), \\\n",
    "     ('hasCant',       ('cant','can\\'t')), \\\n",
    "     ('hasCrash',      ('crash',)), \\\n",
    "     ('hasCool',       ('cool',)), \\\n",
    "     ('hasDifficult',  ('difficult',)), \\\n",
    "     ('hasDisaster',   ('disaster',)), \\\n",
    "     ('hasDown',       (' down',)), \\\n",
    "     ('hasDont',       ('dont','don\\'t','do not','does not','doesn\\'t')), \\\n",
    "     ('hasEasy',       (' easy',)), \\\n",
    "     ('hasExclaim',    ('!',)), \\\n",
    "     ('hasExcite',     (' excite',)), \\\n",
    "     ('hasExpense',    ('expense','expensive')), \\\n",
    "     ('hasFail',       (' fail',)), \\\n",
    "     ('hasFast',       (' fast',)), \\\n",
    "     ('hasFix',        (' fix',)), \\\n",
    "     ('hasFree',       (' free',)), \\\n",
    "     ('hasFrowny',     (':(', '):')), \\\n",
    "     ('hasFuck',       ('fuck',)), \\\n",
    "     ('hasGood',       ('good','great')), \\\n",
    "     ('hasHappy',      (' happy',' happi')), \\\n",
    "     ('hasHate',       ('hate',)), \\\n",
    "     ('hasHeart',      ('heart', '<3')), \\\n",
    "     ('hasIssue',      (' issue',)), \\\n",
    "     ('hasIncredible', ('incredible',)), \\\n",
    "     ('hasInterest',   ('interest',)), \\\n",
    "     ('hasLike',       (' like',)), \\\n",
    "     ('hasLol',        (' lol',)), \\\n",
    "     ('hasLove',       ('love','loving')), \\\n",
    "     ('hasLose',       (' lose',)), \\\n",
    "     ('hasNeat',       ('neat',)), \\\n",
    "     ('hasNever',      (' never',)), \\\n",
    "     ('hasNice',       (' nice',)), \\\n",
    "     ('hasPoor',       ('poor',)), \\\n",
    "     ('hasPerfect',    ('perfect',)), \\\n",
    "     ('hasPlease',     ('please',)), \\\n",
    "     ('hasSerious',    ('serious',)), \\\n",
    "     ('hasShit',       ('shit',)), \\\n",
    "     ('hasSlow',       (' slow',)), \\\n",
    "     ('hasSmiley',     (':)', ':D', '(:')), \\\n",
    "     ('hasSuck',       ('suck',)), \\\n",
    "     ('hasTerrible',   ('terrible',)), \\\n",
    "     ('hasThanks',     ('thank',)), \\\n",
    "     ('hasTrouble',    ('trouble',)), \\\n",
    "     ('hasUnhappy',    ('unhapp',)), \\\n",
    "     ('hasWin',        (' win ','winner','winning')), \\\n",
    "     ('hasWinky',      (';)',)), \\\n",
    "     ('hasWow',        ('wow','omg')) ]\n",
    "\n",
    "\n",
    "def make_tweet_nparr( txt ):\n",
    "    \"\"\"\n",
    "    Extract tweet feature vector as NumPy array.\n",
    "    \"\"\"\n",
    "    # result storage\n",
    "    fvec = numpy.empty( len(testFeatures) )\n",
    "\n",
    "    # search for each feature\n",
    "    txtLow = ' ' + txt.lower() + ' '\n",
    "    for i in range( 0, len(testFeatures) ):\n",
    "\n",
    "        key = testFeatures[i][0]\n",
    "\n",
    "        fvec[i] = False\n",
    "        for tstr in testFeatures[i][1]:\n",
    "            fvec[i] = fvec[i] or (txtLow.find(tstr) != -1)\n",
    "\n",
    "    return fvec\n",
    "\n",
    "\n",
    "def make_tweet_dict( txt ):\n",
    "    \"\"\"\n",
    "    Extract tweet feature vector as dictionary.\n",
    "    \"\"\"\n",
    "    txtLow = ' ' + txt.lower() + ' '\n",
    "\n",
    "    # result storage\n",
    "    fvec = {}\n",
    "\n",
    "    # search for each feature\n",
    "    for test in testFeatures:\n",
    "\n",
    "        key = test[0]\n",
    "\n",
    "        fvec[key] = False;\n",
    "        for tstr in test[1]:\n",
    "            fvec[key] = fvec[key] or (txtLow.find(tstr) != -1)\n",
    "\n",
    "    return fvec\n",
    "\n",
    "\n",
    "def tweet_dict_to_nparr( dict ):\n",
    "    \"\"\"\n",
    "    Convert dictionary feature vector to numpy array\n",
    "    \"\"\"\n",
    "    fvec = numpy.empty( len(testFeatures) )\n",
    "\n",
    "    for i in range( 0, len(testFeatures) ):\n",
    "        fvec[i] = dict[ testFeatures[i][0] ]\n",
    "\n",
    "    return fvec\n",
    "\n",
    "\n",
    "def tweet_nparr_to_dict( nparr, use_standard_features=False ):\n",
    "    \"\"\"\n",
    "    Convert NumPy array to dictionary\n",
    "    \"\"\"\n",
    "    fvec = {}\n",
    "\n",
    "    if use_standard_features:\n",
    "        assert len(nparr) == len(testFeatures)\n",
    "        fvec = {}\n",
    "        for i in range( 0, len(nparr) ):\n",
    "            fvec[ testFeatures[i][0] ] = nparr[i]\n",
    "\n",
    "    else:\n",
    "        for i in range( 0, len(nparr) ):\n",
    "            fvec[ str(i) ] = nparr[i]\n",
    "\n",
    "    return fvec\n",
    "\n",
    "\n",
    "def is_zero_dict( dict ):\n",
    "    \"\"\"\n",
    "    Identifies empty feature vectors\n",
    "    \"\"\"\n",
    "    has_any_features = False\n",
    "    for key in dict:\n",
    "        has_any_features = has_any_features or dict[key]\n",
    "\n",
    "    return not has_any_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a5e22-05b5-4b5a-966f-4dd7f01b8b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
